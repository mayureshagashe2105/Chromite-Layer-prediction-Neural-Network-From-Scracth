{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"><b><centre>Chromite Layer Classification</centre></b></h1><p></p>\n",
    "<p style=\"font-size:115%;\">In this end-to-end project tutorial, we will train an artificial neural network from scratch as a binary classifier for chromite layers prediction, while understanding the mathematics involved that allows the model to make predictions. Data used for the purpose of training the model can be accessed from <a href=\"https://www.kaggle.com/saurabhshahane/multivariate-geochemical-classification\", target=\"_blank\">here</a>.</p><br>\n",
    "<p><b>NOTE:</b> Only the Following selected features are used for the purpose of training</p><br> <table padding=\"30px\"><tr>\n",
    "    <th>Features</th>\n",
    "    <th>Description</th>\n",
    "    </tr>\n",
    "    <tr><td>Motherhole</td><td>Type of the Mother hole observed in image</td></tr>\n",
    "    <tr><td>Holetype</td><td>Type of the hole observed</td></tr>\n",
    "    <tr><td>DepthFrom</td><td>Depth from the Earth at which first observed</td></tr>\n",
    "    <tr><td>DepthTo</td><td>Final depth at which ore is observed</td></tr>\n",
    "    <tr><td>Cr2O3_%</td><td>Percentage of Cr2O3</td></tr>\n",
    "    <tr><td>FeO_%</td><td>Percentage of FeO</td></tr>\n",
    "    <tr><td>SiO2_%</td><td>Percentage of Si02</td></tr>\n",
    "    <tr><td>MgO_%</td><td>Percentage of MgO</td></tr>\n",
    "    <tr><td>Al2O3_%</td><td>Percentage of Al2O3</td></tr>\n",
    "    <tr><td>CaO_%</td><td>Percentage of CaO</td></tr>\n",
    "    <tr><td>P_%</td><td>Percentage of P</td></tr>\n",
    "    <tr><td>Au_ICP_ppm</td><td>Inductive Coupled Plasma analysis of Au</td></tr>\n",
    "    <tr><td>Pt_ICP_ppm</td><td>Inductive Coupled Plasma analysis of Pt</td></tr>\n",
    "    <tr><td>Pd_ICP_ppm</td><td>Inductive Coupled Plasma analysis of Pd</td></tr>\n",
    "    <tr><td>Rh_ICP_ppm</td><td>Inductive Coupled Plasma analysis of Rh</td></tr>\n",
    "    <tr><td>Ir_ICP_ppm</td><td>Inductive Coupled Plasma analysis of Ir</td></tr>\n",
    "    <tr><td>Ru_ICP_ppm</td><td>Inductive Coupled Plasma analysis of Ru</td></tr>\n",
    "    <tr><td>Filter</td><td>Filter</td></tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Comic sans MS\"><h2><b>Importing Project Dependencies</b></h2><p></p>\n",
    "<p style=\"font-size:115%;\">Now let us start with importing all the necessary modules.</p></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Comic sans MS\"><p style=\"font-size:115%;\">Importing the dataset, for the purpose of analysis and prediction.</p></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ProjectCode BH_ID Motherhole  HoleType  MaxDepth  DepthFrom  DepthTo  \\\n",
      "0       J1103  SC06       SC06  borehole    639.35     619.33   619.36   \n",
      "1       J1098  SC11       SC11  borehole    460.20     397.47   397.53   \n",
      "2       J1492  MD16       MD16  borehole    278.80     151.53   151.69   \n",
      "3       J1474  MD09       MD09  borehole     52.23      48.38    48.78   \n",
      "4       J1097  SC63       SC63  borehole    333.75     330.35   330.59   \n",
      "\n",
      "         Date  Cr2O3_%  FeO_%  ...  CaO_%   P_%  Au_ICP_ppm  Pt_ICP_ppm  \\\n",
      "0  23. Dez 08    34.96  19.29  ...   0.89  0.00        0.01        0.53   \n",
      "1  23. Feb 09    39.64  20.77  ...   0.46  0.01        0.01        1.56   \n",
      "2  17. Sep 13    46.28  20.81  ...   0.80  0.00        0.01        0.04   \n",
      "3  09. Nov 09    39.53  19.65  ...   2.20  0.01        0.01        0.10   \n",
      "4  23. Feb 09    43.11  23.51  ...   0.43  0.00        0.01        0.55   \n",
      "\n",
      "   Pd_ICP_ppm  Rh_ICP_ppm  Ir_ICP_ppm  Ru_ICP_ppm  Stratigraphy  Filter  \n",
      "0        0.16        0.14        0.09        0.32           LG1       0  \n",
      "1        0.60        0.42        0.13        0.38           LG2       0  \n",
      "2        0.02        0.10        0.04        0.26           LG3       0  \n",
      "3        0.02        0.07        0.04        0.34           LG4       0  \n",
      "4        0.19        0.21        0.08        0.47           LG5       0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('DataSet_Thaba_Classification.csv', sep=';')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Comic sans MS\"><h2>Data Preprocessing</h2><p></p>\n",
    "<p style=\"font-size:115%;\">Data preprocessing plays a crucial role in any kind of Machine Learning, Deep Learning project. It becomes very necessary to shape and tune the data in order to fit it according to our training needs and criteria, it is the most important part of any project! If such unprocessed data is used for training the model, the unwanted noise from the data could result in the poor performace of the model in real world applications.</p><p style=\"font-size:115%;\">Firstly we will analyze the basic structure of our data</p></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1205 entries, 0 to 1204\n",
      "Data columns (total 23 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   ProjectCode   1205 non-null   object \n",
      " 1   BH_ID         1205 non-null   object \n",
      " 2   Motherhole    1205 non-null   object \n",
      " 3   HoleType      1205 non-null   object \n",
      " 4   MaxDepth      1205 non-null   float64\n",
      " 5   DepthFrom     1205 non-null   float64\n",
      " 6   DepthTo       1205 non-null   float64\n",
      " 7   Date          1205 non-null   object \n",
      " 8   Cr2O3_%       1126 non-null   float64\n",
      " 9   FeO_%         1125 non-null   float64\n",
      " 10  SiO2_%        1126 non-null   float64\n",
      " 11  MgO_%         1126 non-null   float64\n",
      " 12  Al2O3_%       1125 non-null   float64\n",
      " 13  CaO_%         1126 non-null   float64\n",
      " 14  P_%           1125 non-null   float64\n",
      " 15  Au_ICP_ppm    1205 non-null   float64\n",
      " 16  Pt_ICP_ppm    1205 non-null   float64\n",
      " 17  Pd_ICP_ppm    1205 non-null   float64\n",
      " 18  Rh_ICP_ppm    1205 non-null   float64\n",
      " 19  Ir_ICP_ppm    1205 non-null   float64\n",
      " 20  Ru_ICP_ppm    1205 non-null   float64\n",
      " 21  Stratigraphy  1205 non-null   object \n",
      " 22  Filter        1205 non-null   int64  \n",
      "dtypes: float64(16), int64(1), object(6)\n",
      "memory usage: 216.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Comic sans MS\"><p style=\"font-size:115%;\">As we can see some of our chosen features are not mathematically computable, so now we will convert those data columns into mathematically computable features.</p><p style=\"font-size:115%;\"> Null values in our dataset can be a potential threat for poor performance of the model, in order to treat null values, we will replace the null values with the mean of the repective feature column.</p></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    1205\n",
      "Name: ProjectCode, dtype: int64\n",
      "False    1205\n",
      "Name: BH_ID, dtype: int64\n",
      "False    1205\n",
      "Name: Motherhole, dtype: int64\n",
      "False    1205\n",
      "Name: HoleType, dtype: int64\n",
      "False    1205\n",
      "Name: MaxDepth, dtype: int64\n",
      "False    1205\n",
      "Name: DepthFrom, dtype: int64\n",
      "False    1205\n",
      "Name: DepthTo, dtype: int64\n",
      "False    1205\n",
      "Name: Date, dtype: int64\n",
      "False    1205\n",
      "Name: Cr2O3_%, dtype: int64\n",
      "False    1205\n",
      "Name: FeO_%, dtype: int64\n",
      "False    1205\n",
      "Name: SiO2_%, dtype: int64\n",
      "False    1205\n",
      "Name: MgO_%, dtype: int64\n",
      "False    1205\n",
      "Name: Al2O3_%, dtype: int64\n",
      "False    1205\n",
      "Name: CaO_%, dtype: int64\n",
      "False    1205\n",
      "Name: P_%, dtype: int64\n",
      "False    1205\n",
      "Name: Au_ICP_ppm, dtype: int64\n",
      "False    1205\n",
      "Name: Pt_ICP_ppm, dtype: int64\n",
      "False    1205\n",
      "Name: Pd_ICP_ppm, dtype: int64\n",
      "False    1205\n",
      "Name: Rh_ICP_ppm, dtype: int64\n",
      "False    1205\n",
      "Name: Ir_ICP_ppm, dtype: int64\n",
      "False    1205\n",
      "Name: Ru_ICP_ppm, dtype: int64\n",
      "False    1205\n",
      "Name: Stratigraphy, dtype: int64\n",
      "False    1205\n",
      "Name: Filter, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1205 entries, 0 to 1204\n",
      "Data columns (total 23 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   ProjectCode   1205 non-null   object \n",
      " 1   BH_ID         1205 non-null   object \n",
      " 2   Motherhole    1205 non-null   object \n",
      " 3   HoleType      1205 non-null   object \n",
      " 4   MaxDepth      1205 non-null   float64\n",
      " 5   DepthFrom     1205 non-null   float64\n",
      " 6   DepthTo       1205 non-null   float64\n",
      " 7   Date          1205 non-null   object \n",
      " 8   Cr2O3_%       1205 non-null   float64\n",
      " 9   FeO_%         1205 non-null   float64\n",
      " 10  SiO2_%        1205 non-null   float64\n",
      " 11  MgO_%         1205 non-null   float64\n",
      " 12  Al2O3_%       1205 non-null   float64\n",
      " 13  CaO_%         1205 non-null   float64\n",
      " 14  P_%           1205 non-null   float64\n",
      " 15  Au_ICP_ppm    1205 non-null   float64\n",
      " 16  Pt_ICP_ppm    1205 non-null   float64\n",
      " 17  Pd_ICP_ppm    1205 non-null   float64\n",
      " 18  Rh_ICP_ppm    1205 non-null   float64\n",
      " 19  Ir_ICP_ppm    1205 non-null   float64\n",
      " 20  Ru_ICP_ppm    1205 non-null   float64\n",
      " 21  Stratigraphy  1205 non-null   bool   \n",
      " 22  Filter        1205 non-null   int64  \n",
      "dtypes: bool(1), float64(16), int64(1), object(5)\n",
      "memory usage: 208.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#We will convert our target class into either 0 and 1\n",
    "y = np.array(data['Stratigraphy'])\n",
    "data['Stratigraphy'] = data['Stratigraphy'].str.startswith('L')\n",
    "#checking for null values, and replacing it with mean of that features column\n",
    "for i in data.columns:\n",
    "    if (data[i].isnull()).any():\n",
    "        data[i].replace(np.nan, data[i].mean(), inplace=True)\n",
    "\n",
    "for i in data.columns:\n",
    "    print(data[i].isnull().value_counts())\n",
    "\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat2num(arr):\n",
    "    \"\"\"Returns a dictionary of categorical unique values and assigns a categorical integer value to unique categories\n",
    "    \n",
    "    args-\n",
    "        arr = A numpy array of features to be converted into categorical values\n",
    "        \n",
    "    returns-\n",
    "        di = A dictionary of unique features and categorical values associated with it\n",
    "        \n",
    "    \"\"\"\n",
    "    di = dict.fromkeys(arr)\n",
    "    val = np.arange(0, len(di) - 1)\n",
    "    i = 0\n",
    "    for j in di:\n",
    "        di[j] = i\n",
    "        i += 1\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1205 entries, 0 to 1204\n",
      "Data columns (total 23 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   ProjectCode   1205 non-null   object \n",
      " 1   BH_ID         1205 non-null   object \n",
      " 2   Motherhole    1205 non-null   int64  \n",
      " 3   HoleType      1205 non-null   int64  \n",
      " 4   MaxDepth      1205 non-null   float64\n",
      " 5   DepthFrom     1205 non-null   float64\n",
      " 6   DepthTo       1205 non-null   float64\n",
      " 7   Date          1205 non-null   object \n",
      " 8   Cr2O3_%       1205 non-null   float64\n",
      " 9   FeO_%         1205 non-null   float64\n",
      " 10  SiO2_%        1205 non-null   float64\n",
      " 11  MgO_%         1205 non-null   float64\n",
      " 12  Al2O3_%       1205 non-null   float64\n",
      " 13  CaO_%         1205 non-null   float64\n",
      " 14  P_%           1205 non-null   float64\n",
      " 15  Au_ICP_ppm    1205 non-null   float64\n",
      " 16  Pt_ICP_ppm    1205 non-null   float64\n",
      " 17  Pd_ICP_ppm    1205 non-null   float64\n",
      " 18  Rh_ICP_ppm    1205 non-null   float64\n",
      " 19  Ir_ICP_ppm    1205 non-null   float64\n",
      " 20  Ru_ICP_ppm    1205 non-null   float64\n",
      " 21  Stratigraphy  1205 non-null   int64  \n",
      " 22  Filter        1205 non-null   int64  \n",
      "dtypes: float64(16), int64(4), object(3)\n",
      "memory usage: 216.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cat2numdict = {'Motherhole': np.nan, 'HoleType': np.nan, 'Stratigraphy': np.nan}\n",
    "dict1 = cat2num(data['Motherhole'])\n",
    "dict2 = cat2num(data['HoleType'])\n",
    "dict3 = cat2num(data['Stratigraphy'])\n",
    "cat2numdict['Motherhole'] = dict1\n",
    "cat2numdict['HoleType'] = dict2\n",
    "cat2numdict['Stratigraphy'] = dict3\n",
    "data.replace(cat2numdict, inplace=True)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data.drop(['ProjectCode', 'BH_ID', 'Date', 'Stratigraphy'], axis=1)).astype('float32')\n",
    "y = np.array(data['Stratigraphy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StandardScalar(arr):\n",
    "    \"\"\"Scales all the features in the range of -1 to 1, i.e normalization of the features\n",
    "    \n",
    "    args-\n",
    "        arr = A 1D or 2D numpy array of features\n",
    "        \n",
    "    returns-\n",
    "        arr = Normalized feature values with mean == 0 and standard_deviation == 1, for each feature column\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        for i in range(arr.shape[1]):\n",
    "            mean = arr[:, i].mean()\n",
    "            std = arr[:, i].std()\n",
    "            arr[:, i] = (arr[:, i] - mean) / std\n",
    "\n",
    "    except IndexError:\n",
    "        mean = arr.mean()\n",
    "        std = arr.std()\n",
    "        arr = (arr - mean) / std\n",
    "\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.54689 62.549202\n",
      "-4.4320135e-08 0.99999994\n"
     ]
    }
   ],
   "source": [
    "print(X[:, 0].mean(), X[:, 0].std())\n",
    "X = StandardScalar(X)\n",
    "print(X[:, 0].mean(), X[:, 0].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, testing_size=0.2):\n",
    "    \"\"\"Splits the data into training and testing sets\n",
    "    \n",
    "    args-\n",
    "        X = A 1D or 2D numpy array of features\n",
    "        y = A 1D numpy array of target classes\n",
    "        testing_size(deflaut_param) = (default value = 0.2) Percent data to be splitted as testing data\n",
    "        \n",
    "    returns-\n",
    "        X_train = A numpy array of feature training set, of the size (1 - testing_size)% of the actual data\n",
    "        y_train = A numpy array of target classes training set, of the size (1 - testing_size)% of the actual data\n",
    "        X_test = A numpy array of feature testing/validating set, of the size (testing_size)% of the actual data\n",
    "        y_test = A numpy array of target classes testing/validating set, of the size (testing_size)% of the actual data\n",
    "        \n",
    "    \"\"\"\n",
    "    total_rows_no = X.shape[0]\n",
    "    testin_rows_no = int(testing_size * total_rows_no)\n",
    "    rand_row_no = np.random.randint(0, total_rows_no, testin_rows_no)\n",
    "\n",
    "    X_train = np.array(X[rand_row_no])\n",
    "    X_test = np.delete(X, rand_row_no, axis=0)\n",
    "\n",
    "    y_train = np.array(y[rand_row_no])\n",
    "    y_test = np.delete(y, rand_row_no, axis=0)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(X, y, testing_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNeuralNetwork:\n",
    "    def __init__(self, input_nodes, hidden_nodes1, hidden_nodes2, output_nodes, learning_rate=0.001, epochs=1):\n",
    "        \"\"\"Initializes the basic architecture of the Neural Network\n",
    "        \n",
    "        args-\n",
    "            input_nodes = Number of Input nodes in the 1st layer of the network\n",
    "            hidden_nodes1 = Number of noeds in the 1st hidden layer\n",
    "            hidden_nodes2 = Number of nodes in the 2nd hidden layer\n",
    "            output_nodes = Number of target classes to be predicted\n",
    "            learning_rate(defalut_param) = (default_value = 0.001) Learning rate used in backpropogation for \n",
    "                                           tweaking weights and biasess\n",
    "            epochs(defalut_param) = (defalut_value = 1)Number of epochs\n",
    "        \n",
    "        \"\"\"\n",
    "        self.inodes = input_nodes\n",
    "        self.onodes = output_nodes\n",
    "        self.hnodes1 = hidden_nodes1\n",
    "        self.hnodes2 = hidden_nodes2\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.wih1 = np.random.rand(self.hnodes1, self.inodes) - 0.5   # initializing weights associated with 1st hidden layer\n",
    "        self.wh1h2 = np.random.rand(self.hnodes2, self.hnodes1) - 0.5 # initializing weights associated with 2nd hidden layer\n",
    "        self.wh2o = np.random.rand(self.onodes, self.hnodes2) - 0.5   # initializing weights associated with output layer\n",
    "        self.bih1 = np.zeros((self.hnodes1, 1)) + 0.01                # initializing biases associated with 1st hidden layer\n",
    "        self.bh1h2 = np.zeros((self.hnodes2, 1)) + 0.01               # initializing biases associated with 2nd hidden layer\n",
    "        self.bh2o = np.zeros((self.onodes, 1)) + 0.01                 # initializing biases associated with output layer\n",
    "        self.activation_sigmoid = lambda x: scipy.special.expit(x)    # Sigmoid activation function:- 1 / 1 + e^(-x)\n",
    "        self.activation_softmax = lambda x: scipy.special.softmax(x)  # Softmax activation function:- e^(xi) / Sigma(e^xi)\n",
    "        self.loss = 0.0\n",
    "\n",
    "    def forwardprop(self, sample_input):\n",
    "        \"\"\"Passes the sample input through the neural network and produces an output array of probabilities aka \"Feed-forward\"\n",
    "        \n",
    "        args-\n",
    "            sample_input = numpy array of features\n",
    "        \n",
    "        \"\"\"\n",
    "        sample_input = sample_input.reshape(self.batch_size, -1, 1)\n",
    "\n",
    "        hidden_inputs1 = np.matmul(self.wih1, sample_input) + self.bih1 # passing inputs through hidde layer1\n",
    "        hidden_outputs1 = self.activation_sigmoid(hidden_inputs1)       \n",
    "\n",
    "        hidden_inputs2 = np.matmul(self.wh1h2, hidden_outputs1) + self.bh1h2 # passing hidden1 outputs through hidden layer2\n",
    "        hidden_outputs2 = self.activation_sigmoid(hidden_inputs2)\n",
    "\n",
    "        classifier_inputs = np.matmul(self.wh2o, hidden_outputs2) + self.bh2o # passing through final output layer\n",
    "        classifier_outputs = self.activation_softmax(classifier_inputs)\n",
    "\n",
    "        self.outputs = classifier_outputs\n",
    "        self.hidden_outputs1 = hidden_outputs1\n",
    "        self.hidden_outputs2 = hidden_outputs2\n",
    "\n",
    "    def CategoricalCrossEntropy(self, classifier_outputs, sample_output):\n",
    "        \"\"\"Calculates total loss during the training time by the method of \"Categorical-Cross-Entropy\" aka \"Log-Loss\"\n",
    "        \n",
    "        args-\n",
    "            classifier_outputs = output array formulated by forwardprop method\n",
    "            sample_output = actual output for respective sample_input\n",
    "        \n",
    "        formula-\n",
    "            J = 1 / m sigma((yi)log(yi_hat)) where, \n",
    "            m = number of samples tested\n",
    "            yi = actual output for the sample\n",
    "            yi_hat = predicted output array from forwardprop method \n",
    "            J = total loss\n",
    "            \n",
    "        \"\"\"\n",
    "        one_hot_encoded_matrix = np.zeros((self.batch_size, self.onodes, 1)) + 0.01 \n",
    "        one_hot_encoded_matrix[[i for i in range(self.batch_size)], sample_output] = 0.99 # creating one hot encoded matrix\n",
    "\n",
    "        self.one_hot_encoded_matrix = one_hot_encoded_matrix\n",
    "\n",
    "        self.error = np.sum(one_hot_encoded_matrix * np.log(classifier_outputs), axis=0) # Log-Loss\n",
    "        self.error = np.sum(self.error)\n",
    "\n",
    "        self.loss += self.error # Updating loss\n",
    "\n",
    "    def Backprop(self, sample_input, sample_output):\n",
    "        \"\"\"Tweaks the weights and the biases of each layer by Stochastic Gradient Descent\n",
    "        \n",
    "        args-\n",
    "            sample_input = numpy array of features\n",
    "            sample_output = actual output for respective sample_input\n",
    "        \n",
    "        \"\"\"\n",
    "        sample_input = sample_input.reshape(self.batch_size, -1, 1)\n",
    "\n",
    "        \"\"\"gradint of cost function wrt weights of output layer\"\"\"\n",
    "        dc_dwh2o = np.matmul((self.outputs - self.one_hot_encoded_matrix), self.hidden_outputs2.transpose(0, 2, 1))\n",
    "        \n",
    "        \"\"\"gradint of cost function wrt weights of hidden layer2 layer\"\"\"\n",
    "        ao_delta = (self.outputs - self.one_hot_encoded_matrix)\n",
    "\n",
    "\n",
    "        dc_dho = np.matmul(self.wh2o.transpose(), ao_delta) # chain rule\n",
    "        dho_dz = self.hidden_outputs2 * (1 - self.hidden_outputs2) # derivative of hidden_outputs2 with sigmoid as activation\n",
    "        dc_dz = dc_dho * dho_dz\n",
    "        dc_dwh1h2 = np.matmul(dc_dz, self.hidden_outputs1.transpose(0, 2, 1))\n",
    "        \n",
    "        \"\"\"gradint of cost function wrt weights of hidden layer1 layer\"\"\"\n",
    "        dc_dho1 = np.matmul(self.wh1h2.transpose(), dc_dho)\n",
    "        dho_dz1 = self.hidden_outputs1 * (1 - self.hidden_outputs1) # derivative of hidden_outputs1 with sigmoid as activation\n",
    "        dc_dz1 = dc_dho1 * dho_dz1\n",
    "        dc_dwih1 = np.matmul(dc_dz1, sample_input.transpose(0, 2, 1))\n",
    "\n",
    "        dc_dwh2o = np.mean(dc_dwh2o, axis=0)\n",
    "        dc_dwh1h2 = np.mean(dc_dwh1h2, axis=0)\n",
    "        dc_dwih1 = np.mean(dc_dwih1, axis=0)\n",
    "        \n",
    "        \"\"\"Updating weights and biases\"\"\"\n",
    "        self.wh2o -= self.lr * dc_dwh2o\n",
    "        self.wh1h2 -= self.lr * dc_dwh1h2\n",
    "        self.wih1 -= self.lr * dc_dwih1\n",
    "\n",
    "        self.bh2o -= self.lr * np.mean(ao_delta, axis=0)\n",
    "        self.bh1h2 -= self.lr * np.mean(dc_dz, axis=0)\n",
    "        self.bih1 -= self.lr * np.mean(dc_dz1, axis=0)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"trains the model and prints loss and accuracy achieved in every epoch\n",
    "        \n",
    "        args-\n",
    "            X_train = training set of features\n",
    "            y_train = training set of target classes\n",
    "            \n",
    "        \"\"\"\n",
    "        self.batch_size = 1\n",
    "        for i in range(self.epochs):\n",
    "            correct1 = 0\n",
    "            for j in range(X_train.shape[0]):\n",
    "                self.forwardprop(X_train[j])\n",
    "                correct1 += self.score(y_train[j])\n",
    "                self.CategoricalCrossEntropy(self.outputs, y_train[j])\n",
    "                self.Backprop(X_train[j], y_train[j])\n",
    "                #print(j)\n",
    "            acc = correct1 / X_train.shape[0]\n",
    "            print(f'Epoch: {i + 1}/{self.epochs}\\n loss: {-(self.loss / X_train.shape[0])} accuracy: {acc}')\n",
    "            self.loss = 0\n",
    "\n",
    "    def score(self, output_batch):\n",
    "        \"\"\"Calculates the accuracy of the predictions\n",
    "        \n",
    "        args-\n",
    "            output_batch = Actual output batch NumPy array\n",
    "            \n",
    "        returns-\n",
    "            np.sum(correct.astype(int)) = vectorized comparison for correct outputs and converting the array into int array\n",
    "            and taking sum of the array\n",
    "        \n",
    "        \"\"\"\n",
    "        correct_predicted = np.argmax(self.outputs, axis=1).reshape(-1, )\n",
    "        #print(correct_predicted, output_batch)\n",
    "        correct = (correct_predicted == output_batch)\n",
    "        return np.sum(correct.astype(int))\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Testing the model on the test set after fitting the model\n",
    "        \n",
    "        args-\n",
    "            X_test = Testing set of features\n",
    "            y_test = Tesing set of target classes\n",
    "            \n",
    "        \"\"\"\n",
    "        correct1 = 0\n",
    "        for i in range(X_test.shape[0]):\n",
    "            self.forwardprop(X_test[i])\n",
    "            correct1 += self.score(y_test[i])\n",
    "        self.Score = correct1 / X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      " loss: 0.4076499040298143 accuracy: 0.8464730290456431\n",
      "Epoch: 2/5\n",
      " loss: 0.16110872475353785 accuracy: 0.9626556016597511\n",
      "Epoch: 3/5\n",
      " loss: 0.13264514478807155 accuracy: 0.979253112033195\n",
      "Epoch: 4/5\n",
      " loss: 0.11875690171132428 accuracy: 0.983402489626556\n",
      "Epoch: 5/5\n",
      " loss: 0.11026658347217935 accuracy: 0.991701244813278\n"
     ]
    }
   ],
   "source": [
    "model = DenseNeuralNetwork(19, 1024, 512, 2, learning_rate=0.001, epochs=5)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9735772357723578\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "print(model.Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
